{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjpOa3qD+gFbxUVm2S03ur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiroto-noguchi/weekly_articles_2023/blob/main/weekly_article_2023_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcsX6DT3nfCQ"
      },
      "outputs": [],
      "source": [
        "#URLを使用した校正\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = 'https://api.languagetool.org/v2/check'\n",
        "text = 'This are the correct sentences.'\n",
        "params = {\n",
        "    'text': text,\n",
        "    'language': 'en-US'\n",
        "}\n",
        "response = requests.post(url, data=params)\n",
        "result = json.loads(response.text)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ライブラリーを使用した校正\n",
        "!pip install language-tool-python\n",
        "from language_tool_python import LanguageTool\n",
        "\n",
        "text = \"This are the correct sentences.\"\n",
        "tool = LanguageTool('en-US')\n",
        "matches = tool.check(text)\n",
        "matches"
      ],
      "metadata": {
        "id": "nKWoArDropyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for error in matches:\n",
        "  print(error)"
      ],
      "metadata": {
        "id": "YclTyp6yoxXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def translate(text):\n",
        "    endpoint = \"https://api.mymemory.translated.net/get\"\n",
        "    params = {\n",
        "        \"q\": text,\n",
        "        \"langpair\": \"en|ja\"\n",
        "    }\n",
        "    response = requests.get(endpoint, params=params)\n",
        "    data = response.json()\n",
        "    return data\n",
        "\n",
        "text = \"This are the correct sentences.\"\n",
        "translation = translate(text)\n",
        "print(translation['responseData']['translatedText'])"
      ],
      "metadata": {
        "id": "E-xeNIR0o0-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for error in matches:\n",
        "  translation = translate(error.message)\n",
        "  print(translation['responseData']['translatedText'])"
      ],
      "metadata": {
        "id": "iBiVHTe8pY_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/odashi/small_parallel_enja.git\n",
        "\n",
        "with open('/content/small_parallel_enja/train.en.000', 'r', encoding='utf-8') as f:\n",
        "    english_sentences_train = f.readlines()\n",
        "\n",
        "with open('/content/small_parallel_enja/train.ja.000', 'r', encoding='utf-8') as f:\n",
        "    japanese_sentences_train = f.readlines()\n",
        "\n",
        "with open('/content/small_parallel_enja/test.en', 'r', encoding='utf-8') as f:\n",
        "    english_sentences_test = f.readlines()\n",
        "\n",
        "with open('/content/small_parallel_enja/test.ja', 'r', encoding='utf-8') as f:\n",
        "    japanese_sentences_test = f.readlines()"
      ],
      "metadata": {
        "id": "wDjBCVsUpTC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "japanese_sentences_test[:10]"
      ],
      "metadata": {
        "id": "ZPpjUT9OpjLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences_test[:10]"
      ],
      "metadata": {
        "id": "Co-ya9OfplbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# データを変数に与える\n",
        "japanese_sentences = japanese_sentences_train\n",
        "english_sentences = english_sentences_train\n",
        "\n",
        "# 日本語文を整数に変換する\n",
        "japanese_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "japanese_tokenizer.fit_on_texts(japanese_sentences)\n",
        "\n",
        "# 英語文を整数に変換する\n",
        "english_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "\n",
        "# 整数を単語に変換するための逆引き辞書を作成する\n",
        "japanese_index_word = {index: word for word, index in japanese_tokenizer.word_index.items()}\n",
        "english_index_word = {index: word for word, index in english_tokenizer.word_index.items()}\n",
        "\n",
        "# 日本語文を整数に変換する\n",
        "japanese_sequences = japanese_tokenizer.texts_to_sequences(japanese_sentences)\n",
        "\n",
        "# 英語文を整数に変換する\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# パディング(0で埋めることによる文の長さの調整)する\n",
        "japanese_padded = tf.keras.preprocessing.sequence.pad_sequences(japanese_sequences, padding='post')\n",
        "english_padded = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, padding='post')\n",
        "\n",
        "# モデルを定義する\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(len(japanese_tokenizer.word_index)+1, 256),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),\n",
        "    tf.keras.layers.RepeatVector(len(english_padded[0])),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
        "    tf.keras.layers.Dense(len(english_tokenizer.word_index)+1, activation='softmax')\n",
        "])\n",
        "\n",
        "# モデルをコンパイルする\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# モデルを訓練する\n",
        "model.fit(japanese_padded, english_padded, epochs=30)"
      ],
      "metadata": {
        "id": "Eat4tZEvppkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 翻訳したい日本語の文をリストで変数に与える\n",
        "input_sentences = japanese_sentences_test[:20]\n",
        "\n",
        "# 日本語の文を整数に変換する\n",
        "input_sequences = japanese_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "# パディングする\n",
        "input_padded = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, padding='post', maxlen=len(japanese_padded[0]))\n",
        "\n",
        "# 翻訳する\n",
        "output_sequences = model.predict(input_padded)\n",
        "\n",
        "# 翻訳された英語の文を単語に変換する\n",
        "output_sentences = []\n",
        "for seq in output_sequences:\n",
        "    output_sequence = np.argmax(seq, axis=-1)\n",
        "    output_sentence = []\n",
        "    for i in output_sequence:\n",
        "        if i == 0:\n",
        "            break\n",
        "        if i in english_index_word:\n",
        "            output_sentence.append(english_index_word[i])\n",
        "    output_sentence = ' '.join(output_sentence)\n",
        "    output_sentences.append(output_sentence)\n",
        "\n",
        "# 翻訳結果を表示する\n",
        "for i, sentence in enumerate(input_sentences):\n",
        "    print(\"日本語{}：{}\".format(i+1, sentence))\n",
        "    print(\"英語{}：{}\".format(i+1, output_sentences[i]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "SVIVa3Ttp1Dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}