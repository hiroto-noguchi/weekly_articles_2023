{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiroto-noguchi/weekly_articles_2023/blob/main/weekly_article_2023_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Gc4fhQU0Zm6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#事前学習モデルで「あの。」につづく文を生成する。\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "input = tokenizer.encode(\"あの。\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=5)\n",
        "for result in tokenizer.batch_decode(output):\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "2KBETXgL1X5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#すべてのファイルを読み込み、テキストを結合する。\n",
        "import os\n",
        "text_train = ''\n",
        "file_list = os.listdir('/content/')\n",
        "for file in file_list:\n",
        "  if file.startswith('KSJ') and file.endswith('.txt'):\n",
        "    f = open(file)\n",
        "    text_train = text_train + f.read()\n",
        "    f.close()\n",
        "text_train[:1000]"
      ],
      "metadata": {
        "id": "IXwjf1TK3AtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#協力者が複数行にわたって発言している部分のみを抽出する。\n",
        "text = text_train.split('*i: ')\n",
        "extracted_text = ''\n",
        "for line in text:\n",
        "  temp = line.strip().split('\\n')\n",
        "  if len(temp) >= 3:\n",
        "    cont = ''.join(temp[1:])\n",
        "    cont = cont.replace('*s: ','')\n",
        "    cont = cont.replace(' ','')\n",
        "    extracted_text = extracted_text + cont + '\\n'\n",
        "extracted_text[:1000]"
      ],
      "metadata": {
        "id": "3MZckFtg2STf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ファイルに保存する\n",
        "f = open('train.txt', 'w')\n",
        "f.write(extracted_text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "G4LQOZzW3mln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPUの使用を確認する。\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "YnSshlEu3slY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers -b v4.23.1\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "6-Xhr9RU6gXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ファインチューニングする。\n",
        "%%time\n",
        "!python ./transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=3 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --output_dir=output/"
      ],
      "metadata": {
        "id": "7bYbD56h7ZOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ファインチューニングしたモデルを確かめる。\n",
        "from transformers import AutoModelForCausalLM\n",
        "model_rev = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "\n",
        "input = tokenizer.encode(\"あの。\", return_tensors=\"pt\")\n",
        "output = model_rev.generate(input, do_sample=True, max_length=100, num_return_sequences=5)\n",
        "for result in tokenizer.batch_decode(output):\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "u3HdrYHa8buu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}